[[observability-integrations]]
== Observability integrations

Elastic APM supports integrations with other observability solutions.

// remove float tag once other integrations are added
[float]
[[apm-logging-integration]]
=== Logging integration

Many applications use logging frameworks to help record, format, and append an application's logs.
Elastic APM now offers a way to make your application logs even more useful,
by integrating with the most popular logging frameworks in their respective languages.
This means you can easily inject trace information into your logs,
allowing you to explore logs in the Log app and jump straight into the corresponding APM traces -- all while preserving the trace context.

To get started:

. Enable Agent support 
. Ingest your logs into Elasticsearch

[float]
==== Enable Agent support

You must first enable log correlation in your APM Agent(s) and decorate your logs with the `trace.id` or `transaction.id` fields.
Certain Agents even support the `span.id` field.
Some of the Agent's supported logging frameworks will automatically decorate your logs with these fields.
For others, it will need to be done manually.

Take a look at the relevant Agent documentation for further information:

* *Go*: {apm-go-ref-v}/supported-tech.html#supported-tech-logging[Logging frameworks]
* *Java*: {apm-java-ref-v}/config-logging.html#config-enable-log-correlation[`enable_log_correlation`]
// * *.NET*: {apm-dotnet-ref-v}/[]
// * *Node.js*: {apm-node-ref-v}/[]
// * *Python*: {apm-py-ref-v}/[]
// * *Ruby*: {apm-ruby-ref-v}/[]
// * *Rum*: {apm-rum-ref-v}/[]

[float]
==== Ingest your logs into Elasticsearch

Once your logs have been decorated with the appropriate fields, you need to ingest them into Elasticsearch.

Filebeat is Elastic's log shipper.
The {filebeat-ref}/filebeat-getting-started.html[Filebeat getting started]
guide will walk you through the setup process.

Because supported logging frameworks and formats can vary greatly between different programming language,
there is no one-size-fits-all approach for ingesting your logs into Elasticsearch.

For structured logs, 

You'll want to install and run Filebeat.
Send the logs to the same Elastic Stack that is receiving APM data.
Modify the `filebeat.yml` configuration as follows:

* Set the `filebeat.inputs` to point to the source of your logs
* Point Filebeat to Elasticsearch
  * If you're using Elastic cloud, set `cloud.id` and `cloud.auth`.
  * If your using a manual setup, use `output.elasticsearch.hosts`.
* Add a `decode_json_fields` processor, so `processors` looks as follows:

[source,yml]
----
processors:
- add_host_metadata: ~
- decode_json_fields:
    fields: ["message"]
    target: ""
    overwrite_keys: true
----






For unstructured logs, you'll want to add APM identifiers in any easy to parse manner.
In Python, this could be accomplished like this:

[source,python]
----
import logging
format_string = (
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s "
    "| elasticapm "
    "transaction.id=%(elasticapm_transaction_id) "
    "trace.id=%(elasticapm_trace_id) "
    "span.id=%(elasticapm_span_id)"
)
formatter = logging.Formatter(format_string)
----

Then, you'd be able to use Elasticsearch's {ref}/grok-processor.html[Grok Processor]
to extract the structured fields out of the text:

[source, json]
----
{
  "description" : "...",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": ["%{GREEDYDATA:msg} | elasticapm transaction.id=%{DATA:transaction.id} trace.id=%{DATA:trace.id} span.id=%{DATA:span.id}"]
This conversation was marked as resolved by basepi
      }
    }
  ]
}
----

NOTE: Depending on how you've added APM data to your logs,
you may need to tweak this grok pattern in order to work for your setup.
